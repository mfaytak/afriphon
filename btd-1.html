<?xml version="1.0" encoding="utf-8"?>

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"

 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">

<head>
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/solid.css" integrity="sha384-VGP9aw4WtGH/uPAOseYxZ+Vz/vaTb1ehm1bwx92Fm8dTrE+3boLfF1SpAtB1z7HW" crossorigin="anonymous">

  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/fontawesome.css" integrity="sha384-1rquJLNOM3ijoueaaeS5m+McXPJCGdr5HcA03/VHXxcp2kX2sUrQDmFc3jR5i/C7" crossorigin="anonymous">
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

  <meta http-equiv="Content-Style-Type" content="text/css" />

  <meta name="generator" content="pandoc" />

  <meta name="author" content="Banto1d, 23 March 2022" />

  <title>[verb]ing phonetics: workshop 1</title>

  <style type="text/css">

      code{white-space: pre-wrap;}

      span.smallcaps{font-variant: small-caps;}

      span.underline{text-decoration: underline;}

      div.column{display: inline-block; vertical-align: top; width: 50%;}

  </style>

  <link rel="stylesheet" type="text/css" media="screen, projection, print"

    href="assets/styles/slidy.css" />

  <script src="assets/scripts/slidy.js"

    charset="utf-8" type="text/javascript"></script>

</head>

<body>

<div class="slide titlepage">

  <h1 class="title">[verb]ing phonetics: workshop 1</h1>

  <p class="author">

Banto1d, 23 March 2022

  </p>

  <p class="date">Universität Hamburg<br/> Matthew Faytak<br/> Katie Franich</p>

</div>

<div id="overview" class="slide section level2">

<h1>Overview</h1>

<p>Part 1:</p>

<ul>

<li>Assumptions</li>

<li>Why (instrumental) phonetics?</li>

<li>Acoustic data

<ul>

<li>Using Praat</li>

<li>Types of measurements</li>

</ul></li>

</ul>

<p>Part 2:</p>

<ul>

<li>Articulatory data</li>

<li>Data management</li>

<li>Using the data</li>

<li>Open discussion period</li>

</ul>

</div>

<div id="about-the-slides" class="slide section level2">

<h1>About the slides</h1>

<p>These slides are a <strong>web page</strong></p>

<ul>

<li>Use right, left arrow keys to navigate (or click to advance)</li>

<li>Press “A” to see all slides at once, and “A” again to go back to slide view</li>

<li>Links are formatted like <a href="https://www.youtube.com/watch?v=eVaUDAqrpKk">this</a></li>

<li>References look like <span class="cite">This (1985)</span></li>

<li>All references have links provided in the bibliography</li>

</ul>

<p>The slides are hosted <a href="https://github.com/mfaytak/afriphon">here</a> on GitHub</p>

<ul>

<li>Along with all associated media</li>

<li>This slideshow’s URL is <a href="https://mfaytak.github.io/afriphon/btd-1.html">mfaytak.github.io/afriphon/btd-1.html</a></li>

</ul>

</div>

<div id="some-assumptions" class="title-slide slide section level1"><h1>Some assumptions</h1></div><div id="participants" class="slide section level2">

<h1>Participants</h1>

<p>We presume that you are:</p>

<ul>

<li>Employed by, or trained at, a university on the African continent (sub-Sahara)</li>

<li>Researching in a low-resource context

<ul>

<li>External grants are uncommon</li>

<li>Institutional support is low</li>

</ul></li>

<li>Familiar with phonetics in theory, but not necessarily in practice</li>

</ul>

<p>If you are not in this group, we ask that you <strong>prioritize</strong> those in this group for questions and feedback</p>

</div><div id="all-of-us" class="slide section level2">

<h1>All of us</h1>

<p>Let’s assume that we are all committed to:</p>

<ul>

<li>Improving empirical <strong>coverage</strong> of African languages’ sound structures</li>

<li>Building speech data <strong>resources</strong> for African languages

<ul>

<li>Ethos: “some data is better than no data”</li>

<li>Important as starting point for technical development</li>

</ul></li>

<li>Developing <strong>capacity</strong> for an African phonetics practice</li>

<li>Doing these in a way which is adapted to local needs and demands</li>

</ul>

</div>

<div id="why-phonetics" class="title-slide slide section level1"><h1>Why phonetics?</h1></div><div id="definitions" class="slide section level2">

<h1>Definitions</h1>

<p>This workshop is an introduction to <strong>instrumental</strong> phonetics</p>

<ul>

<li>Relying on instrumental readouts for analysis</li>

<li>Not exclusively <strong>impressionistic</strong>: using the ear and transcribing speech sounds</li>

</ul>

<p>“Phonetic” work may also refer to non-contrastive <strong>subphonemic detail</strong></p>

<ul>

<li><em>The specific way</em> a phonemic contrast is produced</li>

<li>This is also important here, but we are focusing on instrumental methods</li>

</ul>

</div><div id="why-instrumental-phonetics" class="slide section level2">

<h1>Why instrumental phonetics?</h1>

<p>Several practical advantages over impressionistic approaches</p>

<ul>

<li><strong>Neutrality</strong> in the face of analytical and perceptual bias</li>

<li><strong>Precision</strong> and reliability in detecting contrasts</li>

<li><strong>Community use</strong> of the created data</li>

</ul>

</div><div id="neutrality" class="slide section level2">

<h1>Neutrality</h1>

<p>In impressionistic phonological description, all presentation of data is filtered through the worker’s theoretical analysis</p>

<ul>

<li>For example: autosegmental representations often make transcriptions more abstract <span class="cite">Hyman (2014):545</span></li>

</ul>

<p><img src="./assets/media/hyman-theory.png" width="700"></p>

<p>Phonetic recordings allow better testing of hypotheses about phonological structure</p>

<ul>

<li>Recordings do not intrinsically involve an analysis, and can be reanalyzed at a later date</li>

<li>Transcribed data (being analyzed) is much harder to use for this purpose</li>

</ul>

</div><div id="neutrality-1" class="slide section level2">

<h1>Neutrality</h1>

<p>Even a trained phonetic ear is prone to making occasional mistakes based on <strong>perceptual bias</strong></p>

<ul>

<li>For example: nasal consonant codas are more often misidentified after non-low vowels <span class="cite">Zee (1981)</span></li>

</ul>

<p><img src="./assets/media/zee.png" width="600"></p>

<ul>

<li>Transcription mistakes permanently enter the record</li>

</ul>

</div><div id="precision" class="slide section level2">

<h1>Precision</h1>

<p>Not all contrasts can be easily described by the analyst’s ear, especially in the moment</p>

<ul>

<li>Fine vowel contrasts (especially central vowels)</li>

<li>Diphthongs versus consonant secondary articulation</li>

<li>Prenasalization versus N+C clusters</li>

<li>Subtle differences in tone level and contour</li>

<li>Multiple downsteps/upsteps</li>

</ul>

<p>Recordings allow for careful listening later</p>

</div><div id="community-use" class="slide section level2">

<h1>Community use</h1>

<p>Recordings are required for instrumental phonetic work: many incidental benefits</p>

<ul>

<li>Speaker community may access the data

<ul>

<li>Literacy development (teaching tools)</li>

<li>Technical development (speech resources)</li>

</ul></li>

<li>Community of scientific researchers may access the data

<ul>

<li>New analyses</li>

<li>Comparative work</li>

</ul></li>

<li>Analysis may be replicated</li>

</ul>

</div><div id="complementary-methods" class="slide section level2">

<h1>Complementary methods</h1>

<p>The aim is not to <em>displace</em> impression-based methods, but to <em>complement</em> them</p>

<ul>

<li>Transcription will always be needed at some level</li>

<li>Our point is that it should not be <em>exclusively</em> relied on as the analytical object</li>

<li>Whenever possible, transcriptions ought to be supplemented with recordings, visualizations of recordings, or instrumental measures as evidence</li>

<li>Instrumental measures as “second opinion” for analysis</li>

</ul>

</div>

<div id="recording-acoustic-data" class="title-slide slide section level1"><h1>Recording acoustic data</h1></div><div id="desired-qualities" class="slide section level2">

<h1>Desired qualities</h1>

<p>We always want acoustic speech data to be:</p>

<ul>

<li>Low in background <strong>noise</strong></li>

<li>Sufficiently <strong>loud</strong> against background noise, but not too loud</li>

<li>Free of <strong>echo</strong></li>

</ul>

<p>Certain details of format are also important:</p>

<ul>

<li>Record using a high <strong>sampling rate</strong>, at least 22.1 kHz</li>

<li>Save in <strong>non-compressed</strong> format (such as <strong>.WAV</strong>; avoid .MP3)</li>

</ul>

</div><div id="good-recording" class="slide section level2">

<h1>Good recording</h1>

<audio id="good" src="./assets/media/best-quality.wav">

</audio>

<p>Here is an example of a good recording</p>

<ul>

<li><button onclick="document.getElementById(&#39;good&#39;).play()">

“La plume de ma tante”

</button></li>

<li>Speaker’s voice is much louder than background, but is not <em>too</em> loud</li>

<li>Background is free of avoidable noise</li>

<li>Practically no echo</li>

</ul>

<p>The following slides contain recordings which fail on one of the points above</p>

</div><div id="too-noisy" class="slide section level2">

<h1>Too noisy</h1>

<p>Recordings should not contain excessive background noise</p>

<audio id="noise2" src="./assets/media/noisy2.wav">

</audio>

<audio id="scuff" src="./assets/media/scuffing.wav">

</audio>

<ul>

<li><button onclick="document.getElementById(&#39;noise2&#39;).play()">

Continuous noise

</button>

from a fan</li>

<li><button onclick="document.getElementById(&#39;scuff&#39;).play()">

Intermittent noise

</button>

from touching the microphone</li>

</ul>

<p>Any noise, however quiet to your ears in the moment, will be much louder in the recording later</p>

</div><div id="how-to-improve" class="slide section level2">

<h1>How to improve</h1>

<p>Listen carefully to your surroundings, and avoid:</p>

<ul>

<li>Rain on the roof (especially metal roofs)</li>

<li>Appliances (refrigerators, any motors or fans)</li>

<li>Busy roads (trucks, taxis)</li>

<li>Chickens, goats, children, etc.</li>

</ul>

<p>Speaker should also minimize non-speech noise:</p>

<ul>

<li>Touching or scratching microphone, or contacting shirt collar</li>

<li>Producing background noises when emphatic (striking chest or table, clapping hands)</li>

<li>Phone ringing or vibrating</li>

</ul>

</div><div id="too-much-echo" class="slide section level2">

<h1>Too much echo</h1>

<audio id="echo1" src="./assets/media/echo1.wav">

</audio>

<audio id="echo2" src="./assets/media/echo2.wav">

</audio>

<p>If echo is strong, speech ends up overlapping itself; problem for listening and analysis later</p>

<ul>

<li><button onclick="document.getElementById(&#39;echo1&#39;).play()">

Slight echo

</button>

(in tiled hallway)</li>

<li><button onclick="document.getElementById(&#39;echo2&#39;).play()">

More echo

</button>

(in concrete stairwell)</li>

</ul>

<p>How to improve: listen for echo and choose surroundings which have less</p>

<ul>

<li>“Soft” rooms reduce echo (couches, carpets, pillows, hanging clothes); tile, stone, and cement produce echo</li>

<li>Record in the back seat of a car (motor off) if available</li>

<li>Record outside if no suitable room exists</li>

</ul>

</div><div id="too-loud-clipped" class="slide section level2">

<h1>Too loud (clipped)</h1>

<audio id="clip" src="./assets/media/clipping.wav">

</audio>

<audio id="pop" src="./assets/media/popping.wav">

</audio>

<p>If the speaker is too loud and/or too close to the microphone, the device cannot respond enough; <strong>clipping</strong> results</p>

<ul>

<li><button onclick="document.getElementById(&#39;clip&#39;).play()">

Clipping of whole utterance

</button></li>

<li>This can also happen for <button onclick="document.getElementById('pop').play()"> stops and fricatives only</button>, where the releases “pop” in the microphone</li>

</ul>

<p>How to improve: make test recordings after you position your microphones</p>

<ul>

<li>If there is general clipping, microphone needs to be further away or speaker needs to be quieter</li>

<li>If stops “pop”, position microphone to the side of the mouth</li>

<li><strong>Gain</strong> can often be adjusted if you are using a recorder</li>

</ul>

</div><div id="equipment" class="slide section level2">

<h1>Equipment</h1>

<p>Not much equipment required: something to make recordings on</p>

<ul>

<li>Laptop computer</li>

<li>Smartphone with recording app

<ul>

<li>“Awesome Voice Recorder X” (free with ads) is a good app</li>

<li>Others must definitely exist</li>

</ul></li>

<li>Hand recorder/memo recorder</li>

<li>Professional recorder (Zoom H4N, etc)</li>

</ul>

<p>A way of transferring files off of the device and storing for future analysis:</p>

<ul>

<li>SD card</li>

<li>USB drive or external drive</li>

</ul>

<p>Headphones, to check recording quality</p>

</div><div id="equipment-1" class="slide section level2">

<h1>Equipment</h1>

<p>External microphones can increase the quality of acoustic data by recording less echo and background noise</p>

<ul>

<li>Look for <strong>cardioid</strong> or <strong>unidirectional</strong> in the description if you are recording single speakers</li>

<li>Recording public events may require an <strong>omnidirectional</strong> microphone</li>

</ul>

<p>An acoustic baffle can reduce echo</p>

<ul>

<li>Soft, fluffy objects: blankets, sofas, etc.</li>

<li>This can also be achieved by selecting the recording setting carefully, and using a cardioid microphone</li>

</ul>

</div><div id="other-tips" class="slide section level2">

<h1>Other tips</h1>

<p>Do not use computer noise reduction/filtering in general</p>

<ul>

<li>Go into your computer’s sound settings and <em>turn this off</em>; make sure apps (i.e. Zoom) don’t have separate filters</li>

<li>Normal to hear slight “fuzz” in background, or very slight echo</li>

</ul>

<p>Recording over <strong>Zoom</strong> works surprisingly well, if all else fails <span class="cite">Ge, Mok, &amp; Xiong (2021); Sanker et al (2021)</span></p>

<ul>

<li>Some <em>small</em> effects on the recording are noted</li>

<li>Turn noise cancellation off if using to record (switch to “original sound”)</li>

<li>Better still to use the speaker’s phone on their end to record, and use Zoom to supervise the process</li>

</ul>

</div>

<div id="basic-praat-tutorial" class="title-slide slide section level1"><h1>Basic Praat (tutorial)</h1></div><div id="downloading" class="slide section level2">

<h1>Downloading</h1>

</div><div id="configuration" class="slide section level2">

<h1>Configuration</h1>

</div><div id="help-pages" class="slide section level2">

<h1>Help pages</h1>

</div><div id="a-first-recording" class="slide section level2">

<h1>A first recording</h1>

<p>Let’s record ourselves saying “xxx” in whichever language we would like</p>

<ul>

<li>Instructions</li>

<li>“Save and close”</li>

</ul>

</div><div id="a-note-on-recording" class="slide section level2">

<h1>A note on recording</h1>

<p>This is not typical for phonetics research</p>

<ul>

<li>Usually we have <em>many more</em> repetitions of the phenomenon at issue</li>

<li>We need this to ensure we can average across enough instances when we come to analysis

<ul>

<li>Human speech production is inherently variable</li>

<li>To get at the actual tendency we need to have many examples of the same thing</li>

</ul></li>

<li>We also prefer to work with <em>multiple speakers</em> whenever possible</li>

</ul>

</div><div id="objects-list" class="slide section level2">

<h1>Objects list</h1>

</div><div id="saving-your-recording" class="slide section level2">

<h1>Saving your recording</h1>

<p>as .WAV (the standard format for phonetics work)</p>

</div><div id="importing-a-sound-file" class="slide section level2">

<h1>Importing a sound file</h1>

<p>Pius sound files</p>

</div>

<div id="understanding-the-data" class="title-slide slide section level1"><h1>Understanding the data</h1></div><div id="waveforms" class="slide section level2">

<h1>Waveforms</h1>

<p>Sound pressure vs. time</p>

<ul>

<li>The pressure the sound waves make on the microphone</li>

<li>We expect any sound signal to <em>oscillate</em></li>

</ul>

<p>Sometimes this is <strong>periodic</strong> (repeating pattern); other times this is <strong>aperiodic</strong> (no clear pattern)</p>

<p>Sounds produced with a more open mouth are <em>louder</em> and have more sound pressure; these are <em>thicker</em> on the waveform</p>

</div><div id="interpreting-waveforms" class="slide section level2">

<h1>Interpreting waveforms</h1>

<p>Durational measures (trivially)</p>

<p>Voicing</p>

</div><div id="spectrograms" class="slide section level2">

<h1>Spectrograms</h1>

<p>We can also show the <strong>spectrogram</strong> for our recording</p>

<ul>

<li>Breaks down our waveform to give us information about the <strong>sound spectrum</strong></li>

</ul>

<p>Spectrograms show time vs. <strong>frequency</strong> vs. sound pressure (color)</p>

<ul>

<li>The darker the region, the more energy there is</li>

</ul>

</div><div id="simple-frequency-examples" class="slide section level2">

<h1>Simple frequency examples</h1>

<p>A sine wave which repeats every 440 ms looks like this:</p>

<p>If we add more sine waves it shows us this:</p>

<p>Think of speech as consisting of <em>many, many</em> of these</p>

</div><div id="interpreting-spectrograms" class="slide section level2">

<h1>Interpreting spectrograms</h1>

<p><strong>Vowels</strong> and <strong>approximants</strong> have characteristic striping, horizontally and vertically</p>

<ul>

<li>Voicing cycle is visible vertically</li>

<li><strong>Formants</strong> are clearly visible horizontally</li>

</ul>

<p><strong>Fricatives</strong> have high-frequency or low-frequency noise</p>

<ul>

<li>Dark smudges across entire portions of spectrogram</li>

<li>The further back the fricative, the lower the frequency</li>

</ul>

<p><strong>Stops</strong> show an absence of (most) energy followed by a <strong>burst</strong> across the whole spectrum</p>

</div><div id="interpreting-spectrograms-1" class="slide section level2">

<h1>Interpreting spectrograms</h1>

<p><strong>Nasals</strong> look somewhat like vowels, with “smudged” formants and less energy (darkness)</p>

</div>

<div id="annotation" class="title-slide slide section level1"><h1>Annotation</h1></div><div id="textgrids" class="slide section level2">

<h1>TextGrids</h1>

</div><div id="interval-tiers" class="slide section level2">

<h1>Interval tiers</h1>

</div><div id="point-tiers" class="slide section level2">

<h1>Point tiers</h1>

</div><div id="example" class="slide section level2">

<h1>Example</h1>

<p>Pius data</p>

</div>

<div id="exporting-data-objects" class="title-slide slide section level1"><h1>Exporting data objects</h1></div><div id="picture-window" class="slide section level2">

<h1>Picture window</h1>

<p>Spectrograms and waveforms can be <strong>drawn</strong> in the picture window</p>

</div><div id="drawing" class="slide section level2">

<h1>Drawing</h1>

</div><div id="painting" class="slide section level2">

<h1>Painting</h1>

</div><div id="garnishing" class="slide section level2">

<h1>“Garnishing”</h1>

</div><div id="adding-textgrid-annotation" class="slide section level2">

<h1>Adding TextGrid annotation</h1>

</div><div id="saving-figures" class="slide section level2">

<h1>Saving figures</h1>

</div>

<div id="duration" class="title-slide slide section level1"><h1>Duration</h1></div><div id="what-is-it-for" class="slide section level2">

<h1>What is it for?</h1>

<p>Duration of segments, subsegments; timing of tones</p>

<p>Laryngeal contrasts (how long is aspiration of stop), gemination, vowel length, etc.</p>

</div><div id="how-to" class="slide section level2">

<h1>How to</h1>

<p>Get times; log as tabular data (t1, t2, dur)</p>

</div><div id="example-1" class="slide section level2">

<h1>Example</h1>

<p>Pius data?</p>

</div>

<div id="pitch-and-intensity" class="title-slide slide section level1"><h1>Pitch and intensity</h1></div><div id="what-are-they-for" class="slide section level2">

<h1>What are they for?</h1>

<p>f0 signals pitch (tone) - all sorts of phenomena</p>

<ul>

<li>Downstep, downdrift, pitch reset, lexical tone contrasts, intonation, etc</li>

</ul>

<p>intensity is useful for a range of properties (cons lenition, stress, etc)</p>

</div><div id="how-to-1" class="slide section level2">

<h1>How to</h1>

<p>Turn on “show pitch”; query pitch (f0)</p>

<p>Exactly parallel for intensity</p>

</div><div id="drawing-figures" class="slide section level2">

<h1>Drawing figures</h1>

<p>f0 and intensity tracks can be added to figures</p>

<ul>

<li>Suggest not overlapping, but rather putting above/below</li>

<li>Annotation with TextGrids like before</li>

</ul>

</div>

<div id="formants" class="title-slide slide section level1"><h1>Formants</h1></div><div id="what-are-they-for-1" class="slide section level2">

<h1>What are they for?</h1>

<p>Formant frequencies provide <strong>vowel quality</strong> and other contrasts</p>

<ul>

<li>F1 inversely relates to height</li>

<li>F2 inversely relates to backness</li>

<li>F3 directly relates to retroflexion, etc.</li>

</ul>

<p>Formant transitions; lateral and nasal quality, etc.</p>

</div><div id="how-to-2" class="slide section level2">

<h1>How to</h1>

<p>Turn on “show formants”</p>

<p>Query formant listing</p>

</div><div id="formant-estimation" class="slide section level2">

<h1>Formant estimation</h1>

<p>Unlike the other measures, formant estimation often requires calibration to individuals</p>

<ul>

<li>Speakers with low f0 need different settings compared to speakers with higher f0</li>

<li>Children (with very high f0) can be especially difficult</li>

<li>Often, different settings are needed for front and back vowels (especially back rounded vowels)</li>

</ul>

<p>Adjust ceiling, adjust number of formants</p>

</div><div id="formant-figures" class="slide section level2">

<h1>Formant figures</h1>

<p>Formants work best in an F1-F2 plot (which is not made in Praat), but formant tracks can be drawn like any other measure</p>

</div><div id="references" class="slide section level2 bib">

<h1>References</h1>

<p>Hyman, L. (2014). How to study a tone language. <em>Language Documentation &amp; Conservation</em>, 8, 525-562. <a href="https://scholarspace.manoa.hawaii.edu/handle/10125/24624">PDF</a></p>

<p>Ge, C., Xiong, Y., &amp; Mok, P. (2021). How reliable are phonetic data collected remotely? Comparison of recording devices and environments on acoustic measurements. In <em>Proc Interspeech 2021</em>, 1683-1687. <a href="http://ling.cuhk.edu.hk/people/peggy/GeXiongMok_Interspeech2021.pdf">PDF</a></p>

<p>Sanker, C., Babinski, S., Burns, R., Evans, M., Johns, J., Kim, J., Smith, S., Weber, N., &amp; Bowern, C. (2021). (Don’t) try this at home! The effects of recording devices and software on phonetic analysis. <em>Language</em>, 97(4), e360-e382. <a href="https://muse.jhu.edu/article/840967/pdf">PDF</a></p>

<p>Zee, E. (1981). Effect of vowel quality on perception of post–vocalic nasal consonants in noise. <em>Journal of Phonetics</em>, 9(1), 35-48. <a href="https://doi.org/10.1016/S0095-4470(19)30925-8">DOI</a></p>

</div>

</body>

</html>
